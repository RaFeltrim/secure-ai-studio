#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üëÅÔ∏è OBSERVABILITY-DRIVEN TESTING FRAMEWORK
SDET Phase 3 Week 11 - Connect Test Results with System Metrics using Prometheus

Enterprise-grade observability integration linking test execution data 
with real-time system metrics for intelligent test decision making.
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Callable, Union
from dataclasses import dataclass, asdict
from pathlib import Path
from datetime import datetime
import json
import subprocess
import time
import uuid
from prometheus_client import CollectorRegistry, Gauge, Counter, Histogram, generate_latest
import requests
from kubernetes import client, config
import statistics

# ==================== PROMETHEUS METRICS INTEGRATION ====================

@dataclass
class TestObservabilityConfig:
    """Configuration for observability-driven testing"""
    prometheus_url: str
    grafana_url: str
    alertmanager_url: str
    metrics_namespace: str = "sdet_testing"
    scrape_interval: str = "15s"
    evaluation_interval: str = "30s"

@dataclass
class ObservableTestResult:
    """Test result with observability context"""
    test_name: str
    status: str
    duration_ms: float
    timestamp: str
    system_metrics: Dict[str, Any]
    correlation_id: str
    environment: str
    resource_utilization: Dict[str, float]

class PrometheusMetricsCollector:
    """Collect and expose test-related metrics to Prometheus"""
    
    def __init__(self, namespace: str = "sdet_testing"):
        self.registry = CollectorRegistry()
        self.namespace = namespace
        self._initialize_metrics()
        
    def _initialize_metrics(self):
        """Initialize Prometheus metrics"""
        
        # Test execution metrics
        self.test_duration = Histogram(
            'test_duration_seconds',
            'Test execution duration in seconds',
            ['test_name', 'environment', 'status'],
            registry=self.registry,
            buckets=(0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0)
        )
        
        self.test_executions = Counter(
            'test_executions_total',
            'Total number of test executions',
            ['test_name', 'environment', 'status'],
            registry=self.registry
        )
        
        self.test_failures = Counter(
            'test_failures_total',
            'Total number of test failures',
            ['test_name', 'environment', 'failure_type'],
            registry=self.registry
        )
        
        # System resource metrics during test execution
        self.cpu_usage = Gauge(
            'test_cpu_usage_percent',
            'CPU usage percentage during test execution',
            ['test_name', 'environment'],
            registry=self.registry
        )
        
        self.memory_usage = Gauge(
            'test_memory_usage_mb',
            'Memory usage in MB during test execution',
            ['test_name', 'environment'],
            registry=self.registry
        )
        
        self.disk_io = Gauge(
            'test_disk_io_bytes',
            'Disk I/O bytes during test execution',
            ['test_name', 'environment'],
            registry=self.registry
        )
        
        self.network_traffic = Gauge(
            'test_network_bytes',
            'Network traffic bytes during test execution',
            ['test_name', 'environment'],
            registry=self.registry
        )
        
        # Correlation metrics
        self.test_system_correlation = Gauge(
            'test_system_correlation_score',
            'Correlation score between test results and system metrics',
            ['test_name', 'metric_type'],
            registry=self.registry
        )
        
    def record_test_execution(self, result: ObservableTestResult):
        """Record test execution metrics"""
        
        # Record duration histogram
        self.test_duration.labels(
            test_name=result.test_name,
            environment=result.environment,
            status=result.status
        ).observe(result.duration_ms / 1000.0)
        
        # Increment execution counter
        self.test_executions.labels(
            test_name=result.test_name,
            environment=result.environment,
            status=result.status
        ).inc()
        
        # Record system metrics
        self.cpu_usage.labels(
            test_name=result.test_name,
            environment=result.environment
        ).set(result.system_metrics.get('cpu_percent', 0))
        
        self.memory_usage.labels(
            test_name=result.test_name,
            environment=result.environment
        ).set(result.system_metrics.get('memory_mb', 0))
        
        self.disk_io.labels(
            test_name=result.test_name,
            environment=result.environment
        ).set(result.system_metrics.get('disk_io_bytes', 0))
        
        self.network_traffic.labels(
            test_name=result.test_name,
            environment=result.environment
        ).set(result.system_metrics.get('network_bytes', 0))
        
        # Record failures if applicable
        if result.status == 'failed':
            failure_type = self._classify_failure(result)
            self.test_failures.labels(
                test_name=result.test_name,
                environment=result.environment,
                failure_type=failure_type
            ).inc()
            
    def _classify_failure(self, result: ObservableTestResult) -> str:
        """Classify failure type based on system metrics"""
        
        system_metrics = result.system_metrics
        duration = result.duration_ms
        
        # Resource exhaustion failures
        if system_metrics.get('cpu_percent', 0) > 90:
            return 'cpu_exhaustion'
        elif system_metrics.get('memory_mb', 0) > 8000:  # 8GB threshold
            return 'memory_exhaustion'
        elif duration > 30000:  # 30 seconds
            return 'timeout'
        else:
            return 'application_error'
            
    def get_metrics_text(self) -> str:
        """Get metrics in Prometheus text format"""
        return generate_latest(self.registry).decode('utf-8')

# ==================== SYSTEM METRICS COLLECTOR ====================

class SystemMetricsCollector:
    """Collect system metrics during test execution"""
    
    def __init__(self, prometheus_config: TestObservabilityConfig):
        self.prometheus_url = prometheus_config.prometheus_url
        self.collected_metrics = []
        
    def collect_during_test(self, test_name: str, duration_seconds: int) -> List[Dict[str, Any]]:
        """Collect system metrics during test execution"""
        
        metrics_series = []
        start_time = time.time()
        
        while time.time() - start_time < duration_seconds:
            try:
                # Collect various system metrics
                metrics = {
                    'timestamp': datetime.now().isoformat(),
                    'test_name': test_name,
                    'cpu_percent': self._get_cpu_usage(),
                    'memory_mb': self._get_memory_usage(),
                    'disk_io_bytes': self._get_disk_io(),
                    'network_bytes': self._get_network_traffic(),
                    'load_average': self._get_load_average(),
                    'process_count': self._get_process_count()
                }
                
                metrics_series.append(metrics)
                self.collected_metrics.append(metrics)
                
                time.sleep(1)  # Collect every second
                
            except Exception as e:
                print(f"Warning: Failed to collect metrics: {e}")
                time.sleep(1)
                
        return metrics_series
        
    def _get_cpu_usage(self) -> float:
        """Get current CPU usage percentage"""
        try:
            # In real implementation, this would query actual system metrics
            # For demo, simulate realistic CPU usage
            import psutil
            return psutil.cpu_percent(interval=0.1)
        except ImportError:
            # Mock implementation
            return 45.0 + (time.time() % 20)  # Vary between 45-65%
            
    def _get_memory_usage(self) -> float:
        """Get current memory usage in MB"""
        try:
            import psutil
            return psutil.virtual_memory().used / 1024 / 1024
        except ImportError:
            # Mock implementation
            return 4000.0 + (time.time() % 2000)  # Vary between 4000-6000 MB
            
    def _get_disk_io(self) -> int:
        """Get disk I/O bytes"""
        try:
            import psutil
            disk_io = psutil.disk_io_counters()
            return disk_io.read_bytes + disk_io.write_bytes if disk_io else 0
        except ImportError:
            # Mock implementation
            return int(1000000 + (time.time() % 5000000))  # 1-6 MB range
            
    def _get_network_traffic(self) -> int:
        """Get network traffic bytes"""
        try:
            import psutil
            net_io = psutil.net_io_counters()
            return net_io.bytes_sent + net_io.bytes_recv if net_io else 0
        except ImportError:
            # Mock implementation
            return int(500000 + (time.time() % 2000000))  # 0.5-2.5 MB range
            
    def _get_load_average(self) -> float:
        """Get system load average"""
        try:
            import os
            load_avg = os.getloadavg()
            return load_avg[0]  # 1-minute load average
        except (ImportError, OSError):
            # Mock implementation
            return 1.5 + (time.time() % 1.0)  # Vary between 1.5-2.5
            
    def _get_process_count(self) -> int:
        """Get current process count"""
        try:
            import psutil
            return len(psutil.pids())
        except ImportError:
            # Mock implementation
            return 150 + int(time.time() % 50)  # 150-200 processes

# ==================== CORRELATION ANALYZER ====================

class TestSystemCorrelationAnalyzer:
    """Analyze correlations between test results and system metrics"""
    
    def __init__(self):
        self.correlation_history = []
        
    def analyze_correlations(self, test_results: List[ObservableTestResult]) -> Dict[str, Any]:
        """Analyze correlations between test outcomes and system metrics"""
        
        analysis = {
            'correlation_matrix': {},
            'anomalies_detected': [],
            'performance_impact_factors': [],
            'recommendations': []
        }
        
        # Group results by test name
        test_groups = {}
        for result in test_results:
            if result.test_name not in test_groups:
                test_groups[result.test_name] = []
            test_groups[result.test_name].append(result)
            
        # Analyze each test group
        for test_name, results in test_groups.items():
            correlation_data = self._analyze_test_correlations(results)
            analysis['correlation_matrix'][test_name] = correlation_data
            
        # Detect system-wide anomalies
        analysis['anomalies_detected'] = self._detect_system_anomalies(test_results)
        
        # Identify performance impact factors
        analysis['performance_impact_factors'] = self._identify_impact_factors(test_results)
        
        # Generate recommendations
        analysis['recommendations'] = self._generate_correlation_recommendations(analysis)
        
        self.correlation_history.append(analysis)
        
        return analysis
        
    def _analyze_test_correlations(self, results: List[ObservableTestResult]) -> Dict[str, float]:
        """Analyze correlations for specific test"""
        
        if len(results) < 2:
            return {'insufficient_data': True}
            
        # Extract metric series
        durations = [r.duration_ms for r in results]
        cpu_usages = [r.system_metrics.get('cpu_percent', 0) for r in results]
        memory_usages = [r.system_metrics.get('memory_mb', 0) for r in results]
        
        # Calculate correlations
        correlations = {
            'duration_vs_cpu': self._calculate_correlation(durations, cpu_usages),
            'duration_vs_memory': self._calculate_correlation(durations, memory_usages),
            'cpu_vs_memory': self._calculate_correlation(cpu_usages, memory_usages)
        }
        
        return correlations
        
    def _calculate_correlation(self, x: List[float], y: List[float]) -> float:
        """Calculate Pearson correlation coefficient"""
        if len(x) != len(y) or len(x) < 2:
            return 0.0
            
        try:
            mean_x = statistics.mean(x)
            mean_y = statistics.mean(y)
            
            numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))
            sum_sq_x = sum((xi - mean_x) ** 2 for xi in x)
            sum_sq_y = sum((yi - mean_y) ** 2 for yi in y)
            
            if sum_sq_x == 0 or sum_sq_y == 0:
                return 0.0
                
            correlation = numerator / (sum_sq_x * sum_sq_y) ** 0.5
            return round(correlation, 3)
            
        except Exception:
            return 0.0
            
    def _detect_system_anomalies(self, results: List[ObservableTestResult]) -> List[Dict[str, Any]]:
        """Detect anomalies in system metrics during testing"""
        
        anomalies = []
        
        # Flatten all metrics
        all_metrics = []
        for result in results:
            metrics = result.system_metrics.copy()
            metrics['test_name'] = result.test_name
            metrics['timestamp'] = result.timestamp
            all_metrics.append(metrics)
            
        if not all_metrics:
            return anomalies
            
        # Analyze each metric type
        metric_types = ['cpu_percent', 'memory_mb', 'disk_io_bytes', 'network_bytes']
        
        for metric_type in metric_types:
            values = [m.get(metric_type, 0) for m in all_metrics if metric_type in m]
            if len(values) < 3:
                continue
                
            mean_val = statistics.mean(values)
            std_val = statistics.stdev(values) if len(values) > 1 else 0
            
            # Detect outliers (more than 2 standard deviations from mean)
            for i, value in enumerate(values):
                if std_val > 0 and abs(value - mean_val) > (2 * std_val):
                    anomalies.append({
                        'type': 'statistical_outlier',
                        'metric': metric_type,
                        'value': value,
                        'mean': mean_val,
                        'std_deviation': std_val,
                        'severity': 'HIGH' if abs(value - mean_val) > (3 * std_val) else 'MEDIUM',
                        'timestamp': all_metrics[i]['timestamp'],
                        'test_name': all_metrics[i]['test_name']
                    })
                    
        return anomalies
        
    def _identify_impact_factors(self, results: List[ObservableTestResult]) -> List[Dict[str, Any]]:
        """Identify system factors that impact test performance"""
        
        impact_factors = []
        
        # Group failed vs passed tests
        failed_tests = [r for r in results if r.status == 'failed']
        passed_tests = [r for r in results if r.status == 'passed']
        
        if not failed_tests or not passed_tests:
            return impact_factors
            
        # Compare system metrics between failed and passed tests
        failed_metrics = {
            'avg_cpu': statistics.mean([r.system_metrics.get('cpu_percent', 0) for r in failed_tests]),
            'avg_memory': statistics.mean([r.system_metrics.get('memory_mb', 0) for r in failed_tests]),
            'avg_duration': statistics.mean([r.duration_ms for r in failed_tests])
        }
        
        passed_metrics = {
            'avg_cpu': statistics.mean([r.system_metrics.get('cpu_percent', 0) for r in passed_tests]),
            'avg_memory': statistics.mean([r.system_metrics.get('memory_mb', 0) for r in passed_tests]),
            'avg_duration': statistics.mean([r.duration_ms for r in passed_tests])
        }
        
        # Identify significant differences
        cpu_diff = abs(failed_metrics['avg_cpu'] - passed_metrics['avg_cpu'])
        memory_diff = abs(failed_metrics['avg_memory'] - passed_metrics['avg_memory'])
        duration_diff = abs(failed_metrics['avg_duration'] - passed_metrics['avg_duration'])
        
        if cpu_diff > 15:  # 15% CPU difference
            impact_factors.append({
                'factor': 'cpu_utilization',
                'impact': 'HIGH' if cpu_diff > 30 else 'MEDIUM',
                'failed_avg': failed_metrics['avg_cpu'],
                'passed_avg': passed_metrics['avg_cpu'],
                'difference': cpu_diff
            })
            
        if memory_diff > 2000:  # 2GB memory difference
            impact_factors.append({
                'factor': 'memory_usage',
                'impact': 'HIGH' if memory_diff > 4000 else 'MEDIUM',
                'failed_avg': failed_metrics['avg_memory'],
                'passed_avg': passed_metrics['avg_memory'],
                'difference': memory_diff
            })
            
        if duration_diff > 5000:  # 5 second duration difference
            impact_factors.append({
                'factor': 'execution_time',
                'impact': 'HIGH' if duration_diff > 10000 else 'MEDIUM',
                'failed_avg': failed_metrics['avg_duration'],
                'passed_avg': passed_metrics['avg_duration'],
                'difference': duration_diff
            })
            
        return impact_factors
        
    def _generate_correlation_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on correlation analysis"""
        
        recommendations = []
        
        # Check for high correlations
        for test_name, correlations in analysis['correlation_matrix'].items():
            if correlations.get('duration_vs_cpu', 0) > 0.7:
                recommendations.append(f"High CPU correlation detected for {test_name} - optimize CPU-intensive operations")
                
            if correlations.get('duration_vs_memory', 0) > 0.7:
                recommendations.append(f"High memory correlation detected for {test_name} - review memory usage patterns")
                
        # Check for anomalies
        high_severity_anomalies = [a for a in analysis['anomalies_detected'] if a['severity'] == 'HIGH']
        if high_severity_anomalies:
            recommendations.append(f"Investigate {len(high_severity_anomalies)} high-severity system anomalies")
            
        # Check impact factors
        high_impact_factors = [f for f in analysis['performance_impact_factors'] if f['impact'] == 'HIGH']
        if high_impact_factors:
            for factor in high_impact_factors:
                recommendations.append(f"Address {factor['factor']} - significant performance impact detected")
                
        return recommendations

# ==================== ALERTING SYSTEM ====================

class ObservabilityAlertManager:
    """Manage alerts based on observability data"""
    
    def __init__(self, alertmanager_url: str):
        self.alertmanager_url = alertmanager_url
        self.active_alerts = []
        
    def evaluate_test_alerts(self, test_result: ObservableTestResult) -> List[Dict[str, Any]]:
        """Evaluate if test result should trigger alerts"""
        
        alerts = []
        
        # Performance degradation alert
        if test_result.duration_ms > 30000:  # 30 seconds
            alerts.append({
                'alertname': 'TestPerformanceDegradation',
                'severity': 'warning',
                'test_name': test_result.test_name,
                'duration_ms': test_result.duration_ms,
                'threshold': 30000,
                'description': f'Test {test_result.test_name} exceeded performance threshold'
            })
            
        # System resource exhaustion alert
        cpu_usage = test_result.system_metrics.get('cpu_percent', 0)
        if cpu_usage > 90:
            alerts.append({
                'alertname': 'TestResourceExhaustion',
                'severity': 'critical',
                'test_name': test_result.test_name,
                'resource': 'cpu',
                'usage': cpu_usage,
                'threshold': 90,
                'description': f'CPU usage {cpu_usage}% during test execution'
            })
            
        memory_usage = test_result.system_metrics.get('memory_mb', 0)
        if memory_usage > 8000:  # 8GB
            alerts.append({
                'alertname': 'TestResourceExhaustion',
                'severity': 'critical',
                'test_name': test_result.test_name,
                'resource': 'memory',
                'usage': memory_usage,
                'threshold': 8000,
                'description': f'Memory usage {memory_usage}MB during test execution'
            })
            
        # Test failure correlation alert
        if test_result.status == 'failed':
            # Check if system metrics indicate resource issues
            if cpu_usage > 80 or memory_usage > 6000:
                alerts.append({
                    'alertname': 'TestFailureSystemCorrelation',
                    'severity': 'warning',
                    'test_name': test_result.test_name,
                    'failure_type': 'resource_related',
                    'description': 'Test failure correlated with high system resource usage'
                })
                
        # Send alerts to Alertmanager
        for alert in alerts:
            self._send_alert_to_alertmanager(alert)
            self.active_alerts.append(alert)
            
        return alerts
        
    def _send_alert_to_alertmanager(self, alert: Dict[str, Any]):
        """Send alert to Alertmanager"""
        try:
            alert_payload = {
                'status': 'firing',
                'labels': alert,
                'annotations': {
                    'summary': alert['description'],
                    'timestamp': datetime.now().isoformat()
                },
                'startsAt': datetime.now().isoformat()
            }
            
            # In real implementation:
            # response = requests.post(f"{self.alertmanager_url}/api/v1/alerts", json=[alert_payload])
            # return response.status_code == 200
            
            print(f"üîî Alert sent: {alert['alertname']} - {alert['severity']}")
            
        except Exception as e:
            print(f"‚ùå Failed to send alert: {e}")

# ==================== OBSERVABILITY TESTING ORCHESTRATOR ====================

class ObservabilityTestingOrchestrator:
    """Orchestrate observability-driven testing workflows"""
    
    def __init__(self, config: TestObservabilityConfig):
        self.config = config
        self.metrics_collector = PrometheusMetricsCollector(config.metrics_namespace)
        self.system_collector = SystemMetricsCollector(config)
        self.correlation_analyzer = TestSystemCorrelationAnalyzer()
        self.alert_manager = ObservabilityAlertManager(config.alertmanager_url)
        self.test_results = []
        
    def run_observability_driven_test_suite(self, test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Run test suite with full observability integration"""
        
        print("üëÅÔ∏è OBSERVABILITY-DRIVEN TESTING SUITE")
        print("=" * 50)
        
        suite_start = datetime.now()
        observable_results = []
        
        # Execute each test with observability
        for test_case in test_cases:
            print(f"\nüß™ Executing: {test_case['name']}")
            
            # Start system metrics collection
            metrics_collector = self.system_collector.collect_during_test(
                test_case['name'], 
                test_case.get('expected_duration', 30)
            )
            
            # Execute test (simulated)
            test_result = self._execute_test_with_observability(test_case, metrics_collector)
            
            # Record metrics
            self.metrics_collector.record_test_execution(test_result)
            
            # Evaluate alerts
            alerts = self.alert_manager.evaluate_test_alerts(test_result)
            
            # Store result
            observable_results.append(test_result)
            self.test_results.append(test_result)
            
            # Display result
            status_icon = "‚úÖ" if test_result.status == 'passed' else "‚ùå"
            print(f"{status_icon} {test_result.status} ({test_result.duration_ms:.0f}ms)")
            if alerts:
                print(f"   üîî {len(alerts)} alerts triggered")
                
        suite_end = datetime.now()
        
        # Analyze correlations
        correlation_analysis = self.correlation_analyzer.analyze_correlations(observable_results)
        
        # Generate suite summary
        summary = self._generate_observability_summary(observable_results, correlation_analysis)
        summary['suite_duration'] = (suite_end - suite_start).total_seconds()
        
        return {
            'test_results': [asdict(r) for r in observable_results],
            'correlation_analysis': correlation_analysis,
            'alerts_triggered': len(self.alert_manager.active_alerts),
            'prometheus_metrics': self.metrics_collector.get_metrics_text(),
            'summary': summary
        }
        
    def _execute_test_with_observability(self, test_case: Dict[str, Any], 
                                       metrics_data: List[Dict[str, Any]]) -> ObservableTestResult:
        """Execute test with observability context"""
        
        start_time = datetime.now()
        
        # Simulate test execution
        execution_time = test_case.get('simulated_duration', 2000)  # ms
        time.sleep(execution_time / 1000.0)
        
        # Determine test outcome (simulated)
        import random
        success_probability = test_case.get('success_probability', 0.9)
        status = 'passed' if random.random() < success_probability else 'failed'
        
        # Aggregate system metrics
        if metrics_data:
            avg_metrics = {
                'cpu_percent': statistics.mean([m.get('cpu_percent', 0) for m in metrics_data]),
                'memory_mb': statistics.mean([m.get('memory_mb', 0) for m in metrics_data]),
                'disk_io_bytes': statistics.mean([m.get('disk_io_bytes', 0) for m in metrics_data]),
                'network_bytes': statistics.mean([m.get('network_bytes', 0) for m in metrics_data])
            }
        else:
            avg_metrics = {
                'cpu_percent': 45.0,
                'memory_mb': 4000.0,
                'disk_io_bytes': 1000000,
                'network_bytes': 500000
            }
            
        return ObservableTestResult(
            test_name=test_case['name'],
            status=status,
            duration_ms=execution_time,
            timestamp=start_time.isoformat(),
            system_metrics=avg_metrics,
            correlation_id=str(uuid.uuid4()),
            environment=test_case.get('environment', 'testing'),
            resource_utilization={
                'peak_cpu': max([m.get('cpu_percent', 0) for m in metrics_data]) if metrics_data else 65.0,
                'peak_memory': max([m.get('memory_mb', 0) for m in metrics_data]) if metrics_data else 6000.0
            }
        )
        
    def _generate_observability_summary(self, results: List[ObservableTestResult],
                                      correlation_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Generate observability-focused test summary"""
        
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.status == 'passed')
        failed_tests = total_tests - passed_tests
        
        # Performance metrics
        avg_duration = statistics.mean([r.duration_ms for r in results])
        p95_duration = sorted([r.duration_ms for r in results])[int(len(results) * 0.95)]
        
        # Resource utilization summary
        avg_cpu = statistics.mean([r.system_metrics.get('cpu_percent', 0) for r in results])
        avg_memory = statistics.mean([r.system_metrics.get('memory_mb', 0) for r in results])
        
        return {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': failed_tests,
            'success_rate': (passed_tests / total_tests) * 100 if total_tests > 0 else 0,
            'performance_metrics': {
                'average_duration_ms': round(avg_duration, 2),
                'p95_duration_ms': round(p95_duration, 2),
                'average_cpu_usage': round(avg_cpu, 2),
                'average_memory_usage_mb': round(avg_memory, 2)
            },
            'correlation_insights': {
                'high_correlations': len([c for c in correlation_analysis['correlation_matrix'].values() 
                                        if any(abs(v) > 0.7 for v in c.values() if isinstance(v, (int, float)))]),
                'anomalies_detected': len(correlation_analysis['anomalies_detected']),
                'impact_factors': len(correlation_analysis['performance_impact_factors'])
            },
            'recommendations': correlation_analysis['recommendations'][:3]  # Top 3 recommendations
        }

# ==================== DEMONSTRATION ====================

def demonstrate_observability_capabilities():
    """Demonstrate observability-driven testing capabilities"""
    
    print("üëÅÔ∏è OBSERVABILITY-DRIVEN TESTING")
    print("=" * 45)
    
    print("\nBEFORE - Traditional Testing:")
    print("""
def test_api():
    start = time.time()
    response = requests.get('/api/endpoint')
    duration = time.time() - start
    assert response.status_code == 200
    # No system context, no correlations
    """)
    
    print("\nAFTER - Observability-Driven Testing:")
    print("""
orchestrator = ObservabilityTestingOrchestrator(config)
test_case = {
    'name': 'api_endpoint_test',
    'expected_duration': 30
}

result = orchestrator.run_observability_driven_test_suite([test_case])
correlations = analyzer.analyze_correlations([result])
alerts = alert_manager.evaluate_test_alerts(result)
    """)
    
    print("\nüéØ OBSERVABILITY CAPABILITIES:")
    print("‚úÖ Real-time System Metrics Collection")
    print("‚úÖ Prometheus Integration")
    print("‚úÖ Test-System Correlation Analysis")
    print("‚úÖ Intelligent Alerting")
    print("‚úÖ Performance Impact Identification")
    print("‚úÖ Root Cause Analysis")

def run_observability_demo():
    """Run complete observability demonstration"""
    
    print("\nüß™ OBSERVABILITY TESTING DEMONSTRATION")
    print("=" * 50)
    
    # Configuration
    config = TestObservabilityConfig(
        prometheus_url="http://localhost:9090",
        grafana_url="http://localhost:3000",
        alertmanager_url="http://localhost:9093"
    )
    
    # Initialize orchestrator
    orchestrator = ObservabilityTestingOrchestrator(config)
    
    # Test cases
    test_cases = [
        {
            'name': 'user_authentication_test',
            'simulated_duration': 1500,
            'success_probability': 0.95,
            'environment': 'staging'
        },
        {
            'name': 'content_generation_test',
            'simulated_duration': 8000,
            'success_probability': 0.85,
            'environment': 'staging'
        },
        {
            'name': 'api_endpoints_test',
            'simulated_duration': 3000,
            'success_probability': 0.90,
            'environment': 'production'
        }
    ]
    
    # Run observability-driven test suite
    results = orchestrator.run_observability_driven_test_suite(test_cases)
    
    summary = results['summary']
    
    print(f"\nüìä OBSERVABILITY RESULTS:")
    print(f"Tests Executed: {summary['total_tests']}")
    print(f"Success Rate: {summary['success_rate']:.1f}%")
    print(f"Alerts Triggered: {results['alerts_triggered']}")
    
    print(f"\nüìà PERFORMANCE METRICS:")
    perf = summary['performance_metrics']
    print(f"  Avg Duration: {perf['average_duration_ms']}ms")
    print(f"  P95 Duration: {perf['p95_duration_ms']}ms")
    print(f"  Avg CPU: {perf['average_cpu_usage']}%")
    print(f"  Avg Memory: {perf['average_memory_usage_mb']}MB")
    
    print(f"\nüîç CORRELATION INSIGHTS:")
    corr = summary['correlation_insights']
    print(f"  High Correlations: {corr['high_correlations']}")
    print(f"  Anomalies Detected: {corr['anomalies_detected']}")
    print(f"  Impact Factors: {corr['impact_factors']}")
    
    if summary['recommendations']:
        print(f"\nüìã TOP RECOMMENDATIONS:")
        for rec in summary['recommendations']:
            print(f"  ‚Ä¢ {rec}")
    
    return results

if __name__ == "__main__":
    demonstrate_observability_capabilities()
    print("\n" + "=" * 55)
    run_observability_demo()